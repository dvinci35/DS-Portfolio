{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./app/Qwen2-1.5B-Instruct-bnb-4bit\")\n",
    "\n",
    "def send_request(text):\n",
    "    url = \"http://0.0.0.0:8080/\"\n",
    "    data = text\n",
    "    response = requests.post(url, data=data, stream=True)\n",
    "    return response\n",
    "\n",
    "def generate_word(conv:list, input_text:str):\n",
    "    conv.append(dict(role=\"user\", content=input_text))\n",
    "\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        conv, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    generated_text = '' \n",
    "    response = send_request(chat)\n",
    "    for chunk in response.iter_content(None):\n",
    "        new_text = chunk.decode('utf-8')\n",
    "        yield new_text\n",
    "        generated_text += new_text\n",
    "        \n",
    "    conv.append(dict(role=\"assistant\", content=generated_text))\n",
    "\n",
    "\n",
    "conv = []\n",
    "\n",
    "while True:\n",
    "    input_text = input(\"INPUT: \")\n",
    "    for word in generate_word(conv, input_text):\n",
    "        print(word, end='', flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# from starlette import requests\n",
    "\n",
    "def send_request(text):\n",
    "    url = \"http://0.0.0.0:8080/\"\n",
    "    # data = text\n",
    "    response = requests.post(url, data=text, stream=True)\n",
    "    print(response)\n",
    "    print(type(response))\n",
    "    return response.text\n",
    "\n",
    "# while True:\n",
    "#     url = \"http://0.0.0.0:8080/\"\n",
    "#     data = input(\"INPUT: \")\n",
    "#     response = Request(url, data=data)\n",
    "    \n",
    "requests.re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = []\n",
    "\n",
    "while True:\n",
    "    conv.append(dict(role=\"user\", content=input(\"Input: \")))\n",
    "\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        conv, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    result = send_request(chat)\n",
    "    \n",
    "    # print(result[0][\"generated_text\"])\n",
    "\n",
    "    # conv.append(dict(role=\"assistant\", content=result[0][\"generated_text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def client():\n",
    "    url = \"http://localhost:8000/\"  # replace with the actual server URL\n",
    "    data = {\"text\": [\n",
    "        {'role': 'user', \"content\":'Hello'},\n",
    "        ]}  # replace with the actual input text\n",
    "\n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                print(json.loads(line)[\"word\"])  # assuming the server returns a JSON object with a \"word\" key\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?<|im_end|>\n",
      "\n",
      "I'm an AI language model designed to answer questions and provide information on various topics. How may I help you today?<|im_end|>\n",
      "\n",
      "As an artificial intelligence, my primary goal is to provide assistance and support to users in order to make their lives easier and more efficient. By answering questions, providing information, and assisting with tasks, I can help people achieve their goals and improve their overall experience. Additionally, as part of my programming, I am constantly learning and improving, so any feedback or suggestions from users will help me become even better at serving them.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, pipeline\n",
    "from threading import Thread\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./app/Qwen2-1.5B-Instruct-bnb-4bit\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./app/Qwen2-1.5B-Instruct-bnb-4bit\", device_map = 'auto', torch_dtype = 'auto')\n",
    "pipe = pipeline('text-generation', model = model, tokenizer=tokenizer)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "conv = []\n",
    "while True:\n",
    "    conv.append(dict(role = 'user', content = input()))\n",
    "    input_ids = tokenizer.apply_chat_template(conv, return_tensors=\"pt\", add_generation_prompt=True, tokenize=False)\n",
    "    thread = Thread(target=pipe, kwargs=dict(text_inputs= input_ids, max_new_tokens = 512, streamer = streamer))\n",
    "    thread.start()\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        print(new_text, end = '', flush = True)\n",
    "        generated_text += new_text\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    conv.append(dict(role = 'assistant', content = generated_text))\n",
    "    # generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
